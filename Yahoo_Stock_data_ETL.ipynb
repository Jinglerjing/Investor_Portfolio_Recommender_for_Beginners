{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stock Data Scraping and cleaning (ETL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is to show how to scrap stock data through yfinance, merge and clean them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List of companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Example with 2 sectors\n",
    "data = [\n",
    "    # Technology sector\n",
    "    {\"Sector\": \"Technology\", \"Ticker\": \"AAPL\", \"Company Name\": \"Apple Inc.\"},\n",
    "    {\"Sector\": \"Technology\", \"Ticker\": \"MSFT\", \"Company Name\": \"Microsoft Corporation\"},\n",
    "    {\"Sector\": \"Technology\", \"Ticker\": \"GOOGL\", \"Company Name\": \"Alphabet Inc.\"},\n",
    "    {\"Sector\": \"Technology\", \"Ticker\": \"NVDA\", \"Company Name\": \"NVIDIA Corporation\"},\n",
    "    {\"Sector\": \"Technology\", \"Ticker\": \"ADBE\", \"Company Name\": \"Adobe Inc.\"},\n",
    "\n",
    "    # Healthcare sector\n",
    "    {\"Sector\": \"Healthcare\", \"Ticker\": \"PFE\", \"Company Name\": \"Pfizer Inc.\"},\n",
    "    {\"Sector\": \"Healthcare\", \"Ticker\": \"JNJ\", \"Company Name\": \"Johnson & Johnson\"},\n",
    "    {\"Sector\": \"Healthcare\", \"Ticker\": \"LLY\", \"Company Name\": \"Eli Lilly and Company\"},\n",
    "    {\"Sector\": \"Healthcare\", \"Ticker\": \"MRK\", \"Company Name\": \"Merck & Co., Inc.\"},\n",
    "    {\"Sector\": \"Healthcare\", \"Ticker\": \"ABT\", \"Company Name\": \"Abbott Laboratories\"}\n",
    "    \n",
    "    # Add more names if you'd like\n",
    "]\n",
    "\n",
    "# Convert to DataFrame and save\n",
    "df = pd.DataFrame(data)\n",
    "output_path = \"/mnt/data/stock_sector_list_example.csv\"\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "output_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List of Tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# List of all my tickers\n",
    "tickers = [\n",
    "    #Technology\n",
    "    'AAPL', 'MSFT', 'AMD', 'ADBE', 'AVGO', 'INTC', 'CRM', 'CSCO', 'NVDA', 'ORCL', \n",
    "    'QCOM', 'PLTR', 'IBM', 'NOW', 'TXN', 'CDNS', 'ASML', 'GFS', 'APP', 'ANSS', \n",
    "    \n",
    "    #Healthcare\n",
    "    'ABBV', 'ABT', 'AMGN', 'JNJ', 'PFE', 'LLY', 'MRK', 'GEHC', 'ISRG', 'MDT', \n",
    "    'DHR', 'GILD', 'BIIB', 'DXCM', 'IDXX', 'BMY', 'TMO', 'AZN', 'HCA', 'CVS', \n",
    "    \n",
    "    #Financials\n",
    "    'AIG', 'AXP', 'BAC', 'BK', 'C', 'GS', 'JPM', 'MA', 'MET', 'MS', \n",
    "    'PYPL', 'SCHW', 'USB', 'V', 'BRK.B', 'COF', 'BLK', 'TROW', 'PGR', 'FITB', \n",
    "    \n",
    "    #Consumer Discretionary\n",
    "    'AMZN', 'TSLA', 'HD', 'MCD', 'BKNG', 'GM', 'LOW', 'NKE', 'SBUX', 'TGT', \n",
    "    'DIS', 'ABNB', 'YUM', 'ROST', 'TJX', 'ULTA', 'F', 'EBAY', 'ETSY', 'BBY', \n",
    "    \n",
    "    #Consumer Staples\n",
    "    'CL', 'COST', 'KO', 'MDLZ', 'MO', 'PG', 'PEP', 'WMT', 'KDP', 'CLX', \n",
    "    'KR', 'UNFI', 'GIS', 'TSN', 'SYY', 'ADM', 'KMB', 'EL', 'HSY', 'CHD',\n",
    "    \n",
    "    #Energy\n",
    "    'COP', 'CVX', 'XOM', 'FANG', 'BKR', 'EOG', 'OXY', 'PSX', 'HAL', 'SLB', \n",
    "    'VLO', 'HES', 'DVN', 'MPC', 'APA', 'CPE', 'SM', 'MTDR', 'PDCE',  'PXD',\n",
    "    \n",
    "    #Industrials\n",
    "    'BA', 'CAT', 'DE', 'EMR', 'FDX', 'GD', 'HON', 'LMT', 'UNP', 'UPS', \n",
    "    'CSX', 'ADP', 'MMM', 'RTX', 'GE', 'SWK', 'ITW', 'ETN', 'IR', 'PCAR', \n",
    "    \n",
    "    #Communication Services\n",
    "    'DIS','GOOG', 'GOOGL', 'META', 'CMCSA', 'T', 'TMUS', 'CHTR', 'NFLX', 'VZ', \n",
    "    'SIRI', 'PARA', 'FOXA', 'WBD', 'TTWO', 'ATVI', 'LYV', 'BIDU', 'NTES', 'SPOT', \n",
    "    \n",
    "    #Utilities\n",
    "    'DUK', 'NEE', 'SO', 'EXC', 'CEG', 'XEL', 'AEP', 'ES', 'D', 'NRG', \n",
    "    'PPL', 'PEG', 'ED', 'EVRG', 'EIX', 'WEC', 'AWK', 'ATO', 'SRE', 'CMS', \n",
    "    \n",
    "    #Real Estate\n",
    "    'AMT', 'CSGP', 'SPG', 'WELL', 'O', 'PLD', 'BXP', 'EQIX', 'PSA', 'EQR', \n",
    "    'VNO', 'SLG', 'AVB', 'FRT', 'OHI', 'DLR', 'HST', 'WY', 'IRM', 'ARE', \n",
    "    \n",
    "    #Materials\n",
    "    'LIN', 'DD', 'ECL', 'FCX', 'NEM', 'APD', 'MOS', 'PPG', 'RPM', 'CE', \n",
    "    'EMN', 'ALB', 'VMC', 'CF', 'TREX', 'MLM', 'IFF', 'NUE', 'AVY', 'BALL']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Fetch Meta info of all companies (sector by sector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script allows you to fetch companies' meta info, such as sector, industry, marketcap, dividend yield, etc... you can fetch all the tickers at the same time, or sector by sector and save them individually to your local disk so that you can merge the data with the day trading data (see next script) to do analysis from there. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching metadata for LIN...\n",
      "Fetching metadata for DD...\n",
      "Fetching metadata for ECL...\n",
      "Fetching metadata for FCX...\n",
      "Fetching metadata for NEM...\n",
      "Fetching metadata for APD...\n",
      "Fetching metadata for MOS...\n",
      "Fetching metadata for PPG...\n",
      "Fetching metadata for RPM...\n",
      "Fetching metadata for CE...\n",
      "Fetching metadata for EMN...\n",
      "Fetching metadata for ALB...\n",
      "Fetching metadata for VMC...\n",
      "Fetching metadata for CF...\n",
      "Fetching metadata for TREX...\n",
      "Fetching metadata for MLM...\n",
      "Fetching metadata for IFF...\n",
      "Fetching metadata for NUE...\n",
      "Fetching metadata for AVY...\n",
      "Fetching metadata for BALL...\n",
      "Saved to /Users/xiejing/Desktop/Codeoptest/Personal_Project/Materials/Materials_metadata.csv\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import os\n",
    "    \n",
    "tickers = ['LIN', 'DD', 'ECL', 'FCX', 'NEM', 'APD', 'MOS', 'PPG', 'RPM', 'CE', \n",
    "    'EMN', 'ALB', 'VMC', 'CF', 'TREX', 'MLM', 'IFF', 'NUE', 'AVY', 'BALL']\n",
    "metadata_list = []\n",
    "\n",
    "for ticker in tickers:\n",
    "    print(f\"Fetching metadata for {ticker}...\")\n",
    "    try:\n",
    "        t = yf.Ticker(ticker)\n",
    "        info = t.info\n",
    "        selected_info = {\n",
    "            \"ticker\": ticker,\n",
    "            \"sector\": info.get(\"sector\"),\n",
    "            \"industry\": info.get(\"industry\"),\n",
    "            \"marketCap\": info.get(\"marketCap\"),\n",
    "            \"beta\": info.get(\"beta\"),\n",
    "            \"dividendYield\": info.get(\"dividendYield\"),\n",
    "            \"trailingPE\": info.get(\"trailingPE\"),\n",
    "            \"forwardPE\": info.get(\"forwardPE\"),\n",
    "            \"earningsQuarterlyGrowth\": info.get(\"earningsQuarterlyGrowth\"),\n",
    "            \"fullTimeEmployees\": info.get(\"fullTimeEmployees\"),\n",
    "            \"country\": info.get(\"country\"),\n",
    "            \"website\": info.get(\"website\")\n",
    "        }\n",
    "        metadata_list.append(selected_info)\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving data for {ticker}: {e}\")\n",
    "\n",
    "df = pd.DataFrame(metadata_list)\n",
    "\n",
    "# Save to specified path\n",
    "output_path = r\"/Users/xiejing/Desktop/Codeoptest/Personal_Project/Materials/Materials_metadata.csv\"\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "df.to_csv(output_path, index=False)\n",
    "print(f\"Saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Day Trading Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading LIN...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: LIN.csv\n",
      "Downloading DD...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: DD.csv\n",
      "Downloading ECL...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ECL.csv\n",
      "Downloading FCX...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: FCX.csv\n",
      "Downloading NEM...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: NEM.csv\n",
      "Downloading APD...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: APD.csv\n",
      "Downloading MOS...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: MOS.csv\n",
      "Downloading PPG...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: PPG.csv\n",
      "Downloading RPM...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: RPM.csv\n",
      "Downloading CE...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: CE.csv\n",
      "Downloading EMN...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: EMN.csv\n",
      "Downloading ALB...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ALB.csv\n",
      "Downloading VMC...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: VMC.csv\n",
      "Downloading CF...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: CF.csv\n",
      "Downloading TREX...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: TREX.csv\n",
      "Downloading MLM...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: MLM.csv\n",
      "Downloading IFF...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: IFF.csv\n",
      "Downloading NUE...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: NUE.csv\n",
      "Downloading AVY...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: AVY.csv\n",
      "Downloading BALL...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: BALL.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import os\n",
    "\n",
    "tickers = ['LIN', 'DD', 'ECL', 'FCX', 'NEM', 'APD', 'MOS', 'PPG', 'RPM', 'CE', \n",
    "    'EMN', 'ALB', 'VMC', 'CF', 'TREX', 'MLM', 'IFF', 'NUE', 'AVY', 'BALL']\n",
    "save_path = r\"/Users/xiejing/Desktop/Codeoptest/Personal_Project/Materials\"\n",
    "\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "for ticker in tickers:\n",
    "    print(f\"Downloading {ticker}...\")\n",
    "    try:\n",
    "        df = yf.download(ticker, start=\"2020-01-01\", end=\"2025-05-20\")\n",
    "        if not df.empty:\n",
    "            df.to_csv(os.path.join(save_path, f\"{ticker}.csv\"))\n",
    "            print(f\"Saved: {ticker}.csv\")\n",
    "        else:\n",
    "            print(f\"No data for {ticker}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading {ticker}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Merge the trading data (11 sectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved merged CSV to /Users/xiejing/Desktop/Codeoptest/Personal_Project/Materials/20_Materials_Companies_nasdq_trading_data_2020.01-2025.05.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "tickers = ['LIN', 'DD', 'ECL', 'FCX', 'NEM', 'APD', 'MOS', 'PPG', 'RPM', 'CE', \n",
    "    'EMN', 'ALB', 'VMC', 'CF', 'TREX', 'MLM', 'IFF', 'NUE', 'AVY', 'BALL']\n",
    "folder_path = r\"/Users/xiejing/Desktop/Codeoptest/Personal_Project/Materials\"\n",
    "combined = []\n",
    "\n",
    "for ticker in tickers:\n",
    "    file_path = os.path.join(folder_path, f\"{ticker}.csv\")\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        df['Ticker'] = ticker\n",
    "        combined.append(df)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to read {file_path}: {e}\")\n",
    "\n",
    "if combined:\n",
    "    merged_df = pd.concat(combined, ignore_index=True)\n",
    "    output_path = os.path.join(folder_path, \"20_Materials_Companies_nasdq_trading_data_2020.01-2025.05.csv\")\n",
    "    merged_df.to_csv(output_path, index=False)\n",
    "    print(f\"Saved merged CSV to {output_path}\")\n",
    "else:\n",
    "    print(\"No data files found or loaded.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Merge the Meta infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All metadata files merged.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Folder with all metadata CSVs\n",
    "folder_path = \"/Users/xiejing/Desktop/Codeoptest/Investor_Portfolio_Recommender_for_Beginners\"\n",
    "\n",
    "# Load and concatenate\n",
    "metadata_files = [f for f in os.listdir(folder_path) if \"metadata\" in f.lower()]\n",
    "meta_list = []\n",
    "\n",
    "for file in metadata_files:\n",
    "    file_path = os.path.join(folder_path, file)\n",
    "    df = pd.read_csv(file_path)\n",
    "    meta_list.append(df)\n",
    "\n",
    "# Combine all into one DataFrame\n",
    "meta_all = pd.concat(meta_list, ignore_index=True)\n",
    "\n",
    "# Save final metadata\n",
    "meta_all.to_csv(os.path.join(folder_path, \"merged_metadata.csv\"), index=False)\n",
    "print(\"✅ All metadata files merged.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Clean the metadata file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 198 entries, 0 to 197\n",
      "Data columns (total 12 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   ticker                   198 non-null    object \n",
      " 1   sector                   195 non-null    object \n",
      " 2   industry                 195 non-null    object \n",
      " 3   marketCap                195 non-null    float64\n",
      " 4   beta                     195 non-null    float64\n",
      " 5   dividendYield            172 non-null    float64\n",
      " 6   trailingPE               185 non-null    float64\n",
      " 7   forwardPE                195 non-null    float64\n",
      " 8   earningsQuarterlyGrowth  175 non-null    float64\n",
      " 9   fullTimeEmployees        195 non-null    float64\n",
      " 10  country                  195 non-null    object \n",
      " 11  website                  195 non-null    object \n",
      "dtypes: float64(7), object(5)\n",
      "memory usage: 18.7+ KB\n"
     ]
    }
   ],
   "source": [
    "# Read File\n",
    "metadata = pd.read_csv('/Users/xiejing/Desktop/Codeoptest/Investor_Portfolio_Recommender_for_Beginners/merged_metadata.csv')\n",
    "metadata.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ticker                      0\n",
       "sector                      3\n",
       "industry                    3\n",
       "marketCap                   3\n",
       "beta                        3\n",
       "dividendYield              26\n",
       "trailingPE                 13\n",
       "forwardPE                   3\n",
       "earningsQuarterlyGrowth    23\n",
       "fullTimeEmployees           3\n",
       "country                     3\n",
       "website                     3\n",
       "dtype: int64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if value is null in any column\n",
    "\n",
    "metadata.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows where all columns except 'ticker' are NaN\n",
    "metadata_cleaned = metadata.dropna(subset=metadata.columns.difference(['ticker']), how='all')\n",
    "\n",
    "# Reset the index after dropping\n",
    "metadata_cleaned = metadata_cleaned.reset_index(drop=True)\n",
    "\n",
    "# List of columns to impute with \"Nah\"\n",
    "cols_to_fill = [\n",
    "    \"sector\", \"industry\", \"marketCap\", \"beta\", \"dividendYield\",\n",
    "    \"trailingPE\", \"forwardPE\", \"earningsQuarterlyGrowth\",\n",
    "    \"fullTimeEmployees\", \"country\", \"website\"\n",
    "]\n",
    "\n",
    "# Replace NaNs with \"Nah\" in those columns\n",
    "metadata_cleaned[cols_to_fill] = metadata_cleaned[cols_to_fill].fillna(\"NaN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ticker                     0\n",
       "sector                     0\n",
       "industry                   0\n",
       "marketCap                  0\n",
       "beta                       0\n",
       "dividendYield              0\n",
       "trailingPE                 0\n",
       "forwardPE                  0\n",
       "earningsQuarterlyGrowth    0\n",
       "fullTimeEmployees          0\n",
       "country                    0\n",
       "website                    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check again if value is null in any column\n",
    "metadata_cleaned.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Merge the Trading data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All tradingdata files merged.\n"
     ]
    }
   ],
   "source": [
    "# Folder with all metadata CSVs\n",
    "folder_path = \"/Users/xiejing/Desktop/Codeoptest/Investor_Portfolio_Recommender_for_Beginners\"\n",
    "\n",
    "# Load and concatenate\n",
    "trading_files = [f for f in os.listdir(folder_path) if \"trading\" in f.lower()]\n",
    "trading_list = []\n",
    "\n",
    "for file in trading_files:\n",
    "    file_path = os.path.join(folder_path, file)\n",
    "    df = pd.read_csv(file_path)\n",
    "    trading_list.append(df)\n",
    "\n",
    "# Combine all into one DataFrame\n",
    "trading_all = pd.concat(trading_list, ignore_index=True)\n",
    "\n",
    "# Save final metadata\n",
    "trading_all.to_csv(os.path.join(folder_path, \"merged_tradingdata.csv\"), index=False)\n",
    "print(\"✅ All tradingdata files merged.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Clean the Trading data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 288640 entries, 0 to 288639\n",
      "Data columns (total 7 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   Price   288640 non-null  object\n",
      " 1   Close   288425 non-null  object\n",
      " 2   High    288425 non-null  object\n",
      " 3   Low     288425 non-null  object\n",
      " 4   Open    288425 non-null  object\n",
      " 5   Volume  288425 non-null  object\n",
      " 6   Ticker  288640 non-null  object\n",
      "dtypes: object(7)\n",
      "memory usage: 15.4+ MB\n"
     ]
    }
   ],
   "source": [
    "# Read File\n",
    "tradingdata = pd.read_csv('/Users/xiejing/Desktop/Codeoptest/Investor_Portfolio_Recommender_for_Beginners/merged_tradingdata.csv')\n",
    "tradingdata.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Price       0\n",
       "Close     215\n",
       "High      215\n",
       "Low       215\n",
       "Open      215\n",
       "Volume    215\n",
       "Ticker      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tradingdata.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some injected partial header rows are detected and need for removal\n",
    "\n",
    "# Step 1: Remove rows where 'Price' contains 'Ticker' or other header keywords\n",
    "header_keywords = ['Ticker', 'Date', 'Close', 'High', 'Low', 'Open', 'Volume']\n",
    "\n",
    "# Keep rows where 'Price' is not in the header keywords\n",
    "tradingdata_cleaned = tradingdata[~tradingdata['Price'].isin(header_keywords)].copy()\n",
    "\n",
    "# Step 2: Convert 'Price' to datetime and rename to 'Date'\n",
    "tradingdata_cleaned['Price'] = pd.to_datetime(tradingdata_cleaned['Price'], errors='coerce')\n",
    "tradingdata_cleaned.rename(columns={'Price': 'Date'}, inplace=True)\n",
    "\n",
    "# Step 3: Drop rows with invalid or missing Date\n",
    "tradingdata_cleaned = tradingdata_cleaned.dropna(subset=['Date'])\n",
    "\n",
    "# Optional: Convert numeric columns to correct types\n",
    "numeric_cols = ['Close', 'High', 'Low', 'Open', 'Volume']\n",
    "for col in numeric_cols:\n",
    "    if col in tradingdata_cleaned.columns:\n",
    "        tradingdata_cleaned[col] = pd.to_numeric(tradingdata_cleaned[col], errors='coerce')\n",
    "\n",
    "# Step 4: Reset index\n",
    "tradingdata_cleaned = tradingdata_cleaned.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Date      Close       High        Low       Open   Volume Ticker\n",
      "0 2020-01-02  45.015629  45.111296  44.563388  44.911266  3104600    AIG\n",
      "1 2020-01-03  44.667755  44.763423  44.302480  44.554693  2358800    AIG\n",
      "2 2020-01-06  44.702541  44.911267  44.389449  44.467722  2699700    AIG\n",
      "3 2020-01-07  44.450325  44.728628  44.111143  44.659051  4580100    AIG\n",
      "4 2020-01-08  44.972137  45.441776  44.450319  44.450319  4832100    AIG              Date      Close       High        Low       Open   Volume Ticker\n",
      "288205 2025-05-13  93.802017  94.629391  92.825125  94.539672  3675500    CHD\n",
      "288206 2025-05-14  92.865005  93.532882  91.758520  93.363422  2539100    CHD\n",
      "288207 2025-05-15  94.559998  94.639999  92.610001  93.250000  1908300    CHD\n",
      "288208 2025-05-16  95.820000  95.940002  94.250000  94.639999  2058100    CHD\n",
      "288209 2025-05-19  95.970001  96.220001  95.339996  95.820000  2250000    CHD\n"
     ]
    }
   ],
   "source": [
    "print(tradingdata_cleaned.head(), tradingdata_cleaned.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date      0\n",
       "Close     0\n",
       "High      0\n",
       "Low       0\n",
       "Open      0\n",
       "Volume    0\n",
       "Ticker    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if value is null in any column\n",
    "tradingdata_cleaned.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### done"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
