{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stock Data Scraping and cleaning (ETL)\n",
    "\n",
    "“This project automates the extraction, validation, and transformation of sector-specific stock data from Yahoo Finance. It’s designed as the backend ETL pipeline for a future web app that helps beginner investors build a diversified portfolio. The code mimics real-world data workflows, including modular design, error handling, and reusability — with potential for extension to real-time APIs or cloud data sources.”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code is to show how to scrap stock data through yfinance, merge and clean them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List of companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Example with 2 sectors\n",
    "data = [\n",
    "    # Technology sector\n",
    "    {\"Sector\": \"Technology\", \"Ticker\": \"AAPL\", \"Company Name\": \"Apple Inc.\"},\n",
    "    {\"Sector\": \"Technology\", \"Ticker\": \"MSFT\", \"Company Name\": \"Microsoft Corporation\"},\n",
    "    {\"Sector\": \"Technology\", \"Ticker\": \"GOOGL\", \"Company Name\": \"Alphabet Inc.\"},\n",
    "    {\"Sector\": \"Technology\", \"Ticker\": \"NVDA\", \"Company Name\": \"NVIDIA Corporation\"},\n",
    "    {\"Sector\": \"Technology\", \"Ticker\": \"ADBE\", \"Company Name\": \"Adobe Inc.\"},\n",
    "\n",
    "    # Healthcare sector\n",
    "    {\"Sector\": \"Healthcare\", \"Ticker\": \"PFE\", \"Company Name\": \"Pfizer Inc.\"},\n",
    "    {\"Sector\": \"Healthcare\", \"Ticker\": \"JNJ\", \"Company Name\": \"Johnson & Johnson\"},\n",
    "    {\"Sector\": \"Healthcare\", \"Ticker\": \"LLY\", \"Company Name\": \"Eli Lilly and Company\"},\n",
    "    {\"Sector\": \"Healthcare\", \"Ticker\": \"MRK\", \"Company Name\": \"Merck & Co., Inc.\"},\n",
    "    {\"Sector\": \"Healthcare\", \"Ticker\": \"ABT\", \"Company Name\": \"Abbott Laboratories\"}\n",
    "    \n",
    "    # Add more names if you'd like\n",
    "]\n",
    "\n",
    "# Convert to DataFrame and save\n",
    "df = pd.DataFrame(data)\n",
    "output_path = \"/mnt/data/stock_sector_list_example.csv\"\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "output_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List of Tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# List of all my tickers\n",
    "tickers = [\n",
    "    #Technology\n",
    "    'AAPL', 'MSFT', 'AMD', 'ADBE', 'AVGO', 'INTC', 'CRM', 'CSCO', 'NVDA', 'ORCL', \n",
    "    'QCOM', 'PLTR', 'IBM', 'NOW', 'TXN', 'CDNS', 'ASML', 'GFS', 'APP', 'ANSS', \n",
    "    \n",
    "    #Healthcare\n",
    "    'ABBV', 'ABT', 'AMGN', 'JNJ', 'PFE', 'LLY', 'MRK', 'GEHC', 'ISRG', 'MDT', \n",
    "    'DHR', 'GILD', 'BIIB', 'DXCM', 'IDXX', 'BMY', 'TMO', 'AZN', 'HCA', 'CVS', \n",
    "    \n",
    "    #Financials\n",
    "    'AIG', 'AXP', 'BAC', 'BK', 'C', 'GS', 'JPM', 'MA', 'MET', 'MS', \n",
    "    'PYPL', 'SCHW', 'USB', 'V', 'BRK.B', 'COF', 'BLK', 'TROW', 'PGR', 'FITB', \n",
    "    \n",
    "    #Consumer_Cyclical\n",
    "    'AMZN', 'TSLA', 'HD', 'MCD', 'BKNG', 'GM', 'LOW', 'NKE', 'SBUX', 'TGT',\n",
    "    'ABNB', 'YUM', 'ROST', 'TJX', 'ULTA', 'F', 'EBAY', 'ETSY', 'BBY', \n",
    "    \n",
    "    #Consumer_Defensive\n",
    "    'CL', 'COST', 'KO', 'MDLZ', 'MO', 'PG', 'PEP', 'WMT', 'KDP', 'CLX', \n",
    "    'KR', 'UNFI', 'GIS', 'TSN', 'SYY', 'ADM', 'KMB', 'EL', 'HSY', 'CHD',\n",
    "    \n",
    "    #Energy\n",
    "    'COP', 'CVX', 'XOM', 'FANG', 'BKR', 'EOG', 'OXY', 'PSX', 'HAL', 'SLB', \n",
    "    'VLO', 'HES', 'DVN', 'MPC', 'APA', 'CPE', 'SM', 'MTDR', 'PDCE',  'PXD',\n",
    "    \n",
    "    #Industrials\n",
    "    'BA', 'CAT', 'DE', 'EMR', 'FDX', 'GD', 'HON', 'LMT', 'UNP', 'UPS', \n",
    "    'CSX', 'ADP', 'MMM', 'RTX', 'GE', 'SWK', 'ITW', 'ETN', 'IR', 'PCAR', \n",
    "    \n",
    "    #Communication Services\n",
    "    'DIS','GOOG', 'GOOGL', 'META', 'CMCSA', 'T', 'TMUS', 'CHTR', 'NFLX', 'VZ', \n",
    "    'SIRI', 'PARA', 'FOXA', 'WBD', 'TTWO', 'ATVI', 'LYV', 'BIDU', 'NTES', 'SPOT', \n",
    "    \n",
    "    #Utilities\n",
    "    'DUK', 'NEE', 'SO', 'EXC', 'CEG', 'XEL', 'AEP', 'ES', 'D', 'NRG', \n",
    "    'PPL', 'PEG', 'ED', 'EVRG', 'EIX', 'WEC', 'AWK', 'ATO', 'SRE', 'CMS', \n",
    "    \n",
    "    #Real_Estate\n",
    "    'AMT', 'CSGP', 'SPG', 'WELL', 'O', 'PLD', 'BXP', 'EQIX', 'PSA', 'EQR', \n",
    "    'VNO', 'SLG', 'AVB', 'FRT', 'OHI', 'DLR', 'HST', 'WY', 'IRM', 'ARE', \n",
    "    \n",
    "    #Basic_Materials\n",
    "    'LIN', 'DD', 'ECL', 'FCX', 'NEM', 'APD', 'MOS', 'PPG', 'RPM', 'CE', \n",
    "    'EMN', 'ALB', 'VMC', 'CF', 'TREX', 'MLM', 'IFF', 'NUE', 'AVY', 'BALL']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Fetch Meta info of all companies (sector by sector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script allows you to fetch companies' meta info, such as sector, industry, marketcap, dividend yield, etc... you can fetch all the tickers at the same time, or sector by sector and save them individually to your local disk so that you can merge the data with the day trading data (see next script) to do analysis from there. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching metadata for AMZN...\n",
      "Fetching metadata for TSLA...\n",
      "Fetching metadata for HD...\n",
      "Fetching metadata for MCD...\n",
      "Fetching metadata for BKNG...\n",
      "Fetching metadata for GM...\n",
      "Fetching metadata for LOW...\n",
      "Fetching metadata for NKE...\n",
      "Fetching metadata for SBUX...\n",
      "Fetching metadata for TGT...\n",
      "Fetching metadata for DIS...\n",
      "Fetching metadata for ABNB...\n",
      "Fetching metadata for YUM...\n",
      "Fetching metadata for ROST...\n",
      "Fetching metadata for TJX...\n",
      "Fetching metadata for ULTA...\n",
      "Fetching metadata for F...\n",
      "Fetching metadata for EBAY...\n",
      "Fetching metadata for ETSY...\n",
      "Fetching metadata for BBY...\n",
      "Saved to /Users/xiejing/Desktop/Codeoptest/Personal_Project/Consumer_Discretionary/Consumer_Discretionary_metadata.csv\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import os\n",
    "    \n",
    "tickers = ['AMZN', 'TSLA', 'HD', 'MCD', 'BKNG', 'GM', 'LOW', 'NKE', 'SBUX', 'TGT', \n",
    "    'DIS', 'ABNB', 'YUM', 'ROST', 'TJX', 'ULTA', 'F', 'EBAY', 'ETSY', 'BBY']\n",
    "metadata_list = []\n",
    "\n",
    "for ticker in tickers:\n",
    "    print(f\"Fetching metadata for {ticker}...\")\n",
    "    try:\n",
    "        t = yf.Ticker(ticker)\n",
    "        info = t.info\n",
    "        selected_info = {\n",
    "            \"ticker\": ticker,\n",
    "            \"sector\": info.get(\"sector\"),\n",
    "            \"industry\": info.get(\"industry\"),\n",
    "            \"marketCap\": info.get(\"marketCap\"),\n",
    "            \"beta\": info.get(\"beta\"),\n",
    "            \"dividendYield\": info.get(\"dividendYield\"),\n",
    "            \"trailingPE\": info.get(\"trailingPE\"),\n",
    "            \"forwardPE\": info.get(\"forwardPE\"),\n",
    "            \"earningsQuarterlyGrowth\": info.get(\"earningsQuarterlyGrowth\"),\n",
    "            \"fullTimeEmployees\": info.get(\"fullTimeEmployees\"),\n",
    "            \"country\": info.get(\"country\"),\n",
    "            \"website\": info.get(\"website\")\n",
    "        }\n",
    "        metadata_list.append(selected_info)\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving data for {ticker}: {e}\")\n",
    "\n",
    "df = pd.DataFrame(metadata_list)\n",
    "\n",
    "# Save to specified path\n",
    "output_path = r\"/Users/xiejing/Desktop/Codeoptest/Personal_Project/Consumer_Discretionary/Consumer_Discretionary_metadata.csv\"\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "df.to_csv(output_path, index=False)\n",
    "print(f\"Saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Day Trading Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading LIN...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: LIN.csv\n",
      "Downloading DD...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: DD.csv\n",
      "Downloading ECL...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ECL.csv\n",
      "Downloading FCX...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: FCX.csv\n",
      "Downloading NEM...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: NEM.csv\n",
      "Downloading APD...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: APD.csv\n",
      "Downloading MOS...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: MOS.csv\n",
      "Downloading PPG...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: PPG.csv\n",
      "Downloading RPM...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: RPM.csv\n",
      "Downloading CE...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: CE.csv\n",
      "Downloading EMN...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: EMN.csv\n",
      "Downloading ALB...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ALB.csv\n",
      "Downloading VMC...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: VMC.csv\n",
      "Downloading CF...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: CF.csv\n",
      "Downloading TREX...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: TREX.csv\n",
      "Downloading MLM...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: MLM.csv\n",
      "Downloading IFF...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: IFF.csv\n",
      "Downloading NUE...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: NUE.csv\n",
      "Downloading AVY...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: AVY.csv\n",
      "Downloading BALL...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: BALL.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import os\n",
    "\n",
    "tickers = ['LIN', 'DD', 'ECL', 'FCX', 'NEM', 'APD', 'MOS', 'PPG', 'RPM', 'CE', \n",
    "    'EMN', 'ALB', 'VMC', 'CF', 'TREX', 'MLM', 'IFF', 'NUE', 'AVY', 'BALL']\n",
    "save_path = r\"/Users/xiejing/Desktop/Codeoptest/Personal_Project/Materials\"\n",
    "\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "for ticker in tickers:\n",
    "    print(f\"Downloading {ticker}...\")\n",
    "    try:\n",
    "        df = yf.download(ticker, start=\"2020-01-01\", end=\"2025-05-20\")\n",
    "        if not df.empty:\n",
    "            df.to_csv(os.path.join(save_path, f\"{ticker}.csv\"))\n",
    "            print(f\"Saved: {ticker}.csv\")\n",
    "        else:\n",
    "            print(f\"No data for {ticker}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading {ticker}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Merge the trading data (11 sectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved merged CSV to /Users/xiejing/Desktop/Codeoptest/Personal_Project/Materials/20_Materials_Companies_nasdq_trading_data_2020.01-2025.05.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "tickers = ['LIN', 'DD', 'ECL', 'FCX', 'NEM', 'APD', 'MOS', 'PPG', 'RPM', 'CE', \n",
    "    'EMN', 'ALB', 'VMC', 'CF', 'TREX', 'MLM', 'IFF', 'NUE', 'AVY', 'BALL']\n",
    "folder_path = r\"/Users/xiejing/Desktop/Codeoptest/Personal_Project/Materials\"\n",
    "combined = []\n",
    "\n",
    "for ticker in tickers:\n",
    "    file_path = os.path.join(folder_path, f\"{ticker}.csv\")\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        df['Ticker'] = ticker\n",
    "        combined.append(df)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to read {file_path}: {e}\")\n",
    "\n",
    "if combined:\n",
    "    merged_df = pd.concat(combined, ignore_index=True)\n",
    "    output_path = os.path.join(folder_path, \"20_Materials_Companies_nasdq_trading_data_2020.01-2025.05.csv\")\n",
    "    merged_df.to_csv(output_path, index=False)\n",
    "    print(f\"Saved merged CSV to {output_path}\")\n",
    "else:\n",
    "    print(\"No data files found or loaded.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Merge the Meta infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All metadata files merged.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Folder with all metadata CSVs\n",
    "folder_path = \"/Users/xiejing/Desktop/Codeoptest/Investor_Portfolio_Recommender_for_Beginners\"\n",
    "\n",
    "# Load and concatenate\n",
    "metadata_files = [f for f in os.listdir(folder_path) if \"metadata\" in f.lower()]\n",
    "meta_list = []\n",
    "\n",
    "for file in metadata_files:\n",
    "    file_path = os.path.join(folder_path, file)\n",
    "    df = pd.read_csv(file_path)\n",
    "    meta_list.append(df)\n",
    "\n",
    "# Combine all into one DataFrame\n",
    "meta_all = pd.concat(meta_list, ignore_index=True)\n",
    "\n",
    "# Save final metadata\n",
    "meta_all.to_csv(os.path.join(folder_path, \"merged_metadata.csv\"), index=False)\n",
    "print(\"✅ All metadata files merged.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Clean the metadata file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 198 entries, 0 to 197\n",
      "Data columns (total 12 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   ticker                   198 non-null    object \n",
      " 1   sector                   195 non-null    object \n",
      " 2   industry                 195 non-null    object \n",
      " 3   marketCap                195 non-null    float64\n",
      " 4   beta                     195 non-null    float64\n",
      " 5   dividendYield            172 non-null    float64\n",
      " 6   trailingPE               185 non-null    float64\n",
      " 7   forwardPE                195 non-null    float64\n",
      " 8   earningsQuarterlyGrowth  175 non-null    float64\n",
      " 9   fullTimeEmployees        195 non-null    float64\n",
      " 10  country                  195 non-null    object \n",
      " 11  website                  195 non-null    object \n",
      "dtypes: float64(7), object(5)\n",
      "memory usage: 18.7+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Read File\n",
    "metadata = pd.read_csv('/Users/xiejing/Desktop/Codeoptest/Investor_Portfolio_Recommender_for_Beginners/merged_metadata.csv')\n",
    "metadata.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ticker                      0\n",
       "sector                      3\n",
       "industry                    3\n",
       "marketCap                   3\n",
       "beta                        3\n",
       "dividendYield              26\n",
       "trailingPE                 13\n",
       "forwardPE                   3\n",
       "earningsQuarterlyGrowth    23\n",
       "fullTimeEmployees           3\n",
       "country                     3\n",
       "website                     3\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if value is null in any column\n",
    "\n",
    "metadata.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows where all columns except 'ticker' are NaN\n",
    "metadata_cleaned = metadata.dropna(subset=metadata.columns.difference(['ticker']), how='all')\n",
    "\n",
    "# Reset the index after dropping\n",
    "metadata_cleaned = metadata_cleaned.reset_index(drop=True)\n",
    "\n",
    "# List of columns to impute with \"Nah\"\n",
    "cols_to_fill = [\n",
    "    \"sector\", \"industry\", \"marketCap\", \"beta\", \"dividendYield\",\n",
    "    \"trailingPE\", \"forwardPE\", \"earningsQuarterlyGrowth\",\n",
    "    \"fullTimeEmployees\", \"country\", \"website\"\n",
    "]\n",
    "\n",
    "\n",
    "# Replace NaNs with \"Nah\" in those columns\n",
    "metadata_cleaned[cols_to_fill] = metadata_cleaned[cols_to_fill].fillna(\"NaN\")\n",
    "\n",
    "metadata_cleaned['dividendYield'] = pd.to_numeric(metadata_cleaned['dividendYield'], errors='coerce')\n",
    "metadata_cleaned['trailingPE'] = pd.to_numeric(metadata_cleaned['trailingPE'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ticker                     0\n",
       "sector                     0\n",
       "industry                   0\n",
       "marketCap                  0\n",
       "beta                       0\n",
       "dividendYield              0\n",
       "trailingPE                 0\n",
       "forwardPE                  0\n",
       "earningsQuarterlyGrowth    0\n",
       "fullTimeEmployees          0\n",
       "country                    0\n",
       "website                    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check again if value is null in any column\n",
    "metadata_cleaned.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Merge the Trading data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All tradingdata files merged.\n"
     ]
    }
   ],
   "source": [
    "# Folder with all metadata CSVs\n",
    "folder_path = \"/Users/xiejing/Desktop/Codeoptest/Investor_Portfolio_Recommender_for_Beginners\"\n",
    "\n",
    "# Load and concatenate\n",
    "trading_files = [f for f in os.listdir(folder_path) if \"trading\" in f.lower()]\n",
    "trading_list = []\n",
    "\n",
    "for file in trading_files:\n",
    "    file_path = os.path.join(folder_path, file)\n",
    "    df = pd.read_csv(file_path)\n",
    "    trading_list.append(df)\n",
    "\n",
    "# Combine all into one DataFrame\n",
    "trading_all = pd.concat(trading_list, ignore_index=True)\n",
    "\n",
    "# Save final metadata\n",
    "trading_all.to_csv(os.path.join(folder_path, \"merged_tradingdata.csv\"), index=False)\n",
    "print(\"✅ All tradingdata files merged.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Clean the Trading data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 288640 entries, 0 to 288639\n",
      "Data columns (total 7 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   Price   288640 non-null  object\n",
      " 1   Close   288425 non-null  object\n",
      " 2   High    288425 non-null  object\n",
      " 3   Low     288425 non-null  object\n",
      " 4   Open    288425 non-null  object\n",
      " 5   Volume  288425 non-null  object\n",
      " 6   Ticker  288640 non-null  object\n",
      "dtypes: object(7)\n",
      "memory usage: 15.4+ MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Read File\n",
    "tradingdata = pd.read_csv('/Users/xiejing/Desktop/Codeoptest/Investor_Portfolio_Recommender_for_Beginners/merged_tradingdata.csv')\n",
    "tradingdata.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Price       0\n",
       "Close     215\n",
       "High      215\n",
       "Low       215\n",
       "Open      215\n",
       "Volume    215\n",
       "Ticker      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tradingdata.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some injected partial header rows are detected and need for removal\n",
    "\n",
    "# Step 1: Remove rows where 'Price' contains 'Ticker' or other header keywords\n",
    "header_keywords = ['Ticker', 'Date', 'Close', 'High', 'Low', 'Open', 'Volume']\n",
    "\n",
    "# Keep rows where 'Price' is not in the header keywords\n",
    "tradingdata_cleaned = tradingdata[~tradingdata['Price'].isin(header_keywords)].copy()\n",
    "\n",
    "# Step 2: Convert 'Price' to datetime and rename to 'Date'\n",
    "tradingdata_cleaned['Price'] = pd.to_datetime(tradingdata_cleaned['Price'], errors='coerce')\n",
    "tradingdata_cleaned.rename(columns={'Price': 'Date'}, inplace=True)\n",
    "\n",
    "# Step 3: Drop rows with invalid or missing Date\n",
    "tradingdata_cleaned = tradingdata_cleaned.dropna(subset=['Date'])\n",
    "\n",
    "# Optional: Convert numeric columns to correct types\n",
    "numeric_cols = ['Close', 'High', 'Low', 'Open', 'Volume']\n",
    "for col in numeric_cols:\n",
    "    if col in tradingdata_cleaned.columns:\n",
    "        tradingdata_cleaned[col] = pd.to_numeric(tradingdata_cleaned[col], errors='coerce')\n",
    "\n",
    "# Step 4: Reset index\n",
    "tradingdata_cleaned = tradingdata_cleaned.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Date      Close       High        Low       Open   Volume Ticker\n",
      "0 2020-01-02  45.015629  45.111296  44.563388  44.911266  3104600    AIG\n",
      "1 2020-01-03  44.667755  44.763423  44.302480  44.554693  2358800    AIG\n",
      "2 2020-01-06  44.702541  44.911267  44.389449  44.467722  2699700    AIG\n",
      "3 2020-01-07  44.450325  44.728628  44.111143  44.659051  4580100    AIG\n",
      "4 2020-01-08  44.972137  45.441776  44.450319  44.450319  4832100    AIG              Date      Close       High        Low       Open   Volume Ticker\n",
      "288205 2025-05-13  93.802017  94.629391  92.825125  94.539672  3675500    CHD\n",
      "288206 2025-05-14  92.865005  93.532882  91.758520  93.363422  2539100    CHD\n",
      "288207 2025-05-15  94.559998  94.639999  92.610001  93.250000  1908300    CHD\n",
      "288208 2025-05-16  95.820000  95.940002  94.250000  94.639999  2058100    CHD\n",
      "288209 2025-05-19  95.970001  96.220001  95.339996  95.820000  2250000    CHD\n"
     ]
    }
   ],
   "source": [
    "print(tradingdata_cleaned.head(), tradingdata_cleaned.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date      0\n",
       "Close     0\n",
       "High      0\n",
       "Low       0\n",
       "Open      0\n",
       "Volume    0\n",
       "Ticker    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if value is null in any column\n",
    "tradingdata_cleaned.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the cleaned trading data\n",
    "tradingdata_cleaned.to_csv(\"final_cleaned_trading_data.csv\", index=False)\n",
    "\n",
    "# Export the cleaned metadata\n",
    "metadata_cleaned.to_csv(\"final_cleaned_metadata.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Unify columns' naming style in three files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The sector_date\n",
    "\n",
    "# read sector_data file and change all columns' name in lower case\n",
    "sector_data = pd.read_csv('/Users/xiejing/Desktop/Codeoptest/Investor_Portfolio_Recommender_for_Beginners/stock_sector_list_2025.csv')\n",
    "sector_data.columns = [col.lower() for col in sector_data.columns]\n",
    "\n",
    "# Rename a column for easier analysis later\n",
    "sector_data.rename(columns={'company name': 'company_name'}, inplace=True)\n",
    "sector_data['sector'] = sector_data['sector'].replace('conmmunication_services', 'Conmmunication_Services')\n",
    "sector_data['sector'] = sector_data['sector'].replace('real_estate', 'Real_Estate')\n",
    "sector_data['sector'] = sector_data['sector'].replace('Financials', 'Financial_Services')\n",
    "\n",
    "# Save the updated dataset\n",
    "sector_data.to_csv('/Users/xiejing/Desktop/Codeoptest/Investor_Portfolio_Recommender_for_Beginners/stock_sector_list_2025.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The meta_date\n",
    "\n",
    "# read metadata file and change all columns' name in lower case\n",
    "meta_data = pd.read_csv('/Users/xiejing/Desktop/Codeoptest/Investor_Portfolio_Recommender_for_Beginners/final_cleaned_metadata.csv')\n",
    "\n",
    "\n",
    "# Rename a column for easier analysis later\n",
    "meta_data['sector'] = meta_data['sector'].replace('basic_materials', 'Basic_Materials')\n",
    "meta_data['sector'] = meta_data['sector'].replace('consumer_cyclical', 'Consumer_Cyclical')\n",
    "meta_data['sector'] = meta_data['sector'].replace('consumer_defensive', 'Consumer_Defensive')\n",
    "\n",
    "meta_data['sector'] = meta_data['sector'].replace('real_estate', 'Real_Estate')\n",
    "meta_data['sector'] = meta_data['sector'].replace('Financial Services', 'Financial_Services')\n",
    "\n",
    "\n",
    "meta_data.loc[meta_data['ticker'].isin(['AVY', 'BALL']), 'sector'] = 'Basic_Materials'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Read the file\n",
    "meta_data = pd.read_csv('/Users/xiejing/Desktop/Codeoptest/Investor_Portfolio_Recommender_for_Beginners/final_cleaned_metadata.csv')\n",
    "\n",
    "# Step 2: Optional value correction (if needed)\n",
    "meta_data['sector'] = meta_data['sector'].replace('conmmunication_services', 'Communication_Services')\n",
    "\n",
    "# Step 3: Save as comma-separated CSV (new file)\n",
    "meta_data.to_csv('/Users/xiejing/Desktop/Codeoptest/Investor_Portfolio_Recommender_for_Beginners/final_cleaned_metadata.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_data.columns = [col.strip().lower().replace(' ', '_') for col in meta_data.columns]\n",
    "meta_data.columns = meta_data.columns.str.replace('marketcap', 'market_cap') \\\n",
    "                                     .str.replace('dividendyield', 'dividend_yield') \\\n",
    "                                     .str.replace('trailingpe', 'trailing_pe') \\\n",
    "                                     .str.replace('forwardpe', 'forward_pe') \\\n",
    "                                     .str.replace('earningsquarterlygrowth', 'earnings_quarterly_growth') \\\n",
    "                                     .str.replace('fulltimeemployees', 'full_time_employees')\n",
    "\n",
    "\n",
    "meta_data.to_csv('/Users/xiejing/Desktop/Codeoptest/Investor_Portfolio_Recommender_for_Beginners/final_cleaned_metadata.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sector\n",
       "Technology                21\n",
       "Consumer_Defensive        21\n",
       "Industrials               20\n",
       "Real_Estate               20\n",
       "Healthcare                20\n",
       "Utilities                 20\n",
       "Basic_Materials           19\n",
       "Communication_Services    19\n",
       "Consumer_Cyclical         18\n",
       "Financial_Services        18\n",
       "Energy                    17\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_data['sector'].value_counts() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The trading_date\n",
    "\n",
    "# read trading_data file and change all columns' name in lower case\n",
    "trading_data = pd.read_csv('/Users/xiejing/Desktop/Codeoptest/Investor_Portfolio_Recommender_for_Beginners/final_cleaned_trading_data.csv')\n",
    "trading_data.columns = [col.lower() for col in trading_data.columns]\n",
    "\n",
    "trading_data.to_csv('/Users/xiejing/Desktop/Codeoptest/Investor_Portfolio_Recommender_for_Beginners/final_cleaned_trading_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Discrepencies in ticker in three datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sets of tickers from each dataset\n",
    "meta_tickers = set(meta_data['ticker'].dropna().unique())\n",
    "trading_tickers = set(trading_data['ticker'].dropna().unique())\n",
    "sector_tickers = set(sector_data['ticker'].dropna().unique())\n",
    "\n",
    "# Find discrepancies\n",
    "only_in_meta = meta_tickers - trading_tickers - sector_tickers\n",
    "only_in_trading = trading_tickers - meta_tickers - sector_tickers\n",
    "only_in_sector = sector_tickers - meta_tickers - trading_tickers\n",
    "\n",
    "# Show results\n",
    "print(\"Tickers only in meta_data:\", only_in_meta)\n",
    "print(\"Tickers only in trading_data:\", only_in_trading)\n",
    "print(\"Tickers only in sector_data:\", only_in_sector)\n",
    "\n",
    "# Optional: tickers common to all\n",
    "common_all = meta_tickers & trading_tickers & sector_tickers\n",
    "print(\"Tickers common to all three datasets:\", common_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove tickers that are only in sector_data and not in the others\n",
    "sector_data_cleaned = sector_data[~sector_data['ticker'].isin(only_in_sector)]\n",
    "sector_data_cleaned\n",
    "\n",
    "sector_data_cleaned.to_csv('/Users/xiejing/Desktop/Codeoptest/Investor_Portfolio_Recommender_for_Beginners/stock_sector_list_2025.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tickers only in sector_data_cleaned: {'V'}\n",
      "Tickers only in meta_data: set()\n",
      "Tickers in both: {'XEL', 'CSGP', 'GE', 'VNO', 'EMN', 'CLX', 'VZ', 'MDT', 'ARE', 'ETN', 'BBY', 'DHR', 'BAC', 'BALL', 'HD', 'YUM', 'MSFT', 'ED', 'PSA', 'DXCM', 'CMCSA', 'CSX', 'ABNB', 'JPM', 'NVDA', 'MS', 'OXY', 'BMY', 'D', 'PGR', 'BIIB', 'KO', 'CHTR', 'MET', 'DIS', 'GILD', 'BIDU', 'SBUX', 'TJX', 'TREX', 'MCD', 'XOM', 'DLR', 'CL', 'EQR', 'MRK', 'LOW', 'NKE', 'ASML', 'HES', 'TMO', 'CF', 'MPC', 'LIN', 'AWK', 'PPL', 'CRM', 'NTES', 'LLY', 'FITB', 'TSLA', 'SRE', 'WMT', 'PG', 'HSY', 'AMD', 'GFS', 'AVGO', 'ABT', 'BXP', 'UNFI', 'GM', 'SM', 'AZN', 'ROST', 'PLTR', 'RTX', 'MTDR', 'PPG', 'NUE', 'EVRG', 'IRM', 'NFLX', 'BKNG', 'EMR', 'UPS', 'ADBE', 'ULTA', 'EQIX', 'FCX', 'HST', 'PEP', 'NRG', 'CEG', 'AIG', 'SLB', 'SLG', 'SIRI', 'BLK', 'ANSS', 'TXN', 'BA', 'EBAY', 'EXC', 'ADP', 'UNP', 'MO', 'ORCL', 'SPG', 'BKR', 'IBM', 'APP', 'ISRG', 'O', 'FDX', 'ATO', 'AVB', 'GEHC', 'AMZN', 'PARA', 'LYV', 'EL', 'TGT', 'PLD', 'ABBV', 'GOOGL', 'WBD', 'BK', 'MMM', 'GD', 'ADM', 'META', 'IFF', 'MOS', 'JNJ', 'WY', 'T', 'ITW', 'FANG', 'CVS', 'ETSY', 'IDXX', 'OHI', 'MA', 'SCHW', 'ECL', 'LMT', 'COF', 'TTWO', 'AEP', 'WELL', 'KMB', 'CE', 'SWK', 'GS', 'CMS', 'PEG', 'DD', 'AXP', 'CVX', 'PSX', 'F', 'ES', 'KR', 'IR', 'AMT', 'COP', 'TSN', 'MDLZ', 'C', 'WEC', 'HAL', 'FRT', 'VLO', 'TROW', 'SPOT', 'AAPL', 'MLM', 'COST', 'AMGN', 'NEE', 'NEM', 'DUK', 'KDP', 'PCAR', 'CHD', 'CSCO', 'GOOG', 'GIS', 'NOW', 'ALB', 'DVN', 'TMUS', 'VMC', 'SO', 'DE', 'CDNS', 'PYPL', 'AVY', 'APD', 'APA', 'FOXA', 'CAT', 'INTC', 'EIX', 'USB', 'RPM', 'PFE', 'EOG', 'SYY', 'HON', 'QCOM', 'HCA'}\n",
      "Counts — sector: 214 meta: 213 common: 213\n"
     ]
    }
   ],
   "source": [
    "sector_cleaned = pd.read_csv('/Users/xiejing/Desktop/Codeoptest/Investor_Portfolio_Recommender_for_Beginners/stock_sector_list_2025.csv')\n",
    "\n",
    "# Get unique tickers from each dataset\n",
    "sector_cleaned_tickers = set(sector_cleaned['ticker'].dropna().unique())\n",
    "meta_tickers = set(meta_data['ticker'].dropna().unique())\n",
    "\n",
    "# Find differences\n",
    "only_in_sector = sector_cleaned_tickers - meta_tickers\n",
    "only_in_meta = meta_tickers - sector_cleaned_tickers\n",
    "common_tickers = sector_cleaned_tickers & meta_tickers\n",
    "\n",
    "# Print results\n",
    "print(\"Tickers only in sector_data_cleaned:\", only_in_sector)\n",
    "print(\"Tickers only in meta_data:\", only_in_meta)\n",
    "print(\"Tickers in both:\", common_tickers)\n",
    "print(\"Counts — sector:\", len(sector_cleaned_tickers), \"meta:\", len(meta_tickers), \"common:\", len(common_tickers))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      sector ticker company_name\n",
      "69         Consumer_Cyclical    DIS       Disney\n",
      "136  Conmmunication_Services    DIS  Walt Disney\n"
     ]
    }
   ],
   "source": [
    "duplicate_tickers = sector_cleaned['ticker'][sector_cleaned['ticker'].duplicated(keep=False)]\n",
    "print(sector_cleaned[sector_cleaned['ticker'].isin(duplicate_tickers)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the row where ticker is 'DIS' and sector is 'Consumer_Cyclical'\n",
    "sector_cleaned = sector_cleaned[~((sector_cleaned['ticker'] == 'DIS') & (sector_cleaned['sector'] == 'Consumer_Cyclical'))]\n",
    "sector_cleaned = sector_cleaned[~(sector_cleaned['ticker'] == 'V')]\n",
    "\n",
    "sector_cleaned.to_csv('/Users/xiejing/Desktop/Codeoptest/Investor_Portfolio_Recommender_for_Beginners/stock_sector_list_2025.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete duplicated rows in trading data\n",
    "\n",
    "# Step 1: Filter only DIS\n",
    "dis_data = trading_data[trading_data['ticker'] == 'DIS']\n",
    "\n",
    "# Step 2: Drop duplicate dates, keeping the last one\n",
    "dis_data_cleaned = dis_data.drop_duplicates(subset='date', keep='last')\n",
    "\n",
    "# Step 3: Replace DIS rows in the original dataset with cleaned version\n",
    "trading_data_no_dis = trading_data[trading_data['ticker'] != 'DIS']\n",
    "trading_data_final = pd.concat([trading_data_no_dis, dis_data_cleaned], ignore_index=True)\n",
    "\n",
    "# Optional: Save the cleaned dataset\n",
    "trading_data_final.to_csv('/Users/xiejing/Desktop/Codeoptest/Investor_Portfolio_Recommender_for_Beginners/final_cleaned_trading_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates based on ticker + date\n",
    "duplicates = trading_data_final.duplicated(subset=['ticker', 'date'], keep=False)\n",
    "\n",
    "# Show all duplicate rows (if any)\n",
    "duplicate_rows = trading_data_final[duplicates]\n",
    "print(duplicate_rows)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
